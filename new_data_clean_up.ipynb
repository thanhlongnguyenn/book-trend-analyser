{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Testing Data for Regression\n",
    "Here we will clean up the testing data so we evaluate our regression models. \n",
    "\n",
    "The features we require for the regression are (1)Country Frequency (2)ISBN (3)Book Popularity. \n",
    "\n",
    "In order to calculate these features, we require the isbns, ratings and countries of the testing data to be clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first load in the relevant libraries and files\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from textblob import Word\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "\n",
    "nbooks_df = pd.read_csv(\"BX-NewBooks.csv\")\n",
    "nratings_df = pd.read_csv(\"BX-NewBooksRatings.csv\")\n",
    "nusers_df = pd.read_csv(\"BX-NewBooksUsers.csv\")\n",
    "world_countries_s = (pd.read_csv(\"World-Countries.csv\")).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ISBNs\n",
    "Here, we'll remove any records that have an invalid ISBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the ISBN's are valid\n",
    "def validateISBN(isbn: str):\n",
    "    '''\n",
    "    returns True if the isbn is valid and False otherwise\n",
    "    '''\n",
    "    if len(isbn) != 10:\n",
    "        return False\n",
    "    sum = 0\n",
    "    for i in range(9): \n",
    "        if isbn[i].isdigit() and (0 <= int(isbn[i]) <= 9): \n",
    "            sum += int(isbn[i]) * (10 - i)\n",
    "        elif not isbn[i].isdigit():\n",
    "            return False\n",
    "    if isbn[9] == 'X':\n",
    "        sum += 10\n",
    "    elif isbn[9].isdigit():\n",
    "        sum += int(isbn[9])\n",
    "    return (sum % 11 == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Cleaning ISBNs of nbooks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the valid isbns in the new books csv\n",
    "invalid_isbn_condition = (nbooks_df[\"ISBN\"].apply(validateISBN)==False)\n",
    "\n",
    "# Remove the records with invalid isbns \n",
    "nbooks_df = nbooks_df.drop(nbooks_df[invalid_isbn_condition].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning ISBNs of nratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the valid isbns in the new books csv\n",
    "invalid_isbn_condition = (nratings_df[\"ISBN\"].apply(validateISBN)==False)\n",
    "\n",
    "# Remove the records with invalid isbns \n",
    "nratings_df = nratings_df.drop(nratings_df[invalid_isbn_condition].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ratings in nratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 missing ratings\n"
     ]
    }
   ],
   "source": [
    "# let's check if any ratings are misisng\n",
    "print(f\"There are {sum(nratings_df['Book-Rating'].isna())} missing ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 ratings out of the range [0, 10]\n"
     ]
    }
   ],
   "source": [
    "# now let's see if any ratings are outside the range [0, 10]\n",
    "out_of_range_condition = ((nratings_df['Book-Rating']>10) | \n",
    "                          (nratings_df['Book-Rating']<0))\n",
    "print(f\"There are {len(nratings_df[out_of_range_condition])} \"\n",
    "      f\"ratings out of the range [0, 10]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be anything obviously wrong with the ratings data, hence we won't be cleaning it any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean country in nbooks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words(\"english\")) # a set of english stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_stopwords(phrase):\n",
    "      '''\n",
    "      Removes english stop words from a given phrase\n",
    "      Note: phrase must be in all lowercase\n",
    "      '''\n",
    "      # split phrase up into separate words so we can detect stop words\n",
    "      words = phrase.split()\n",
    "      # filter out the stop words \n",
    "      important_words = [word for word in words if word not in STOP_WORDS]\n",
    "      #return the important words\n",
    "      return \" \".join(important_words)\n",
    "      \n",
    "def reformat_word(word):\n",
    "    '''\n",
    "    Reformat words to have all punctuation and trailing space stripped, \n",
    "    turn ampersands into the word \"and\" and lowercase everything\n",
    "    '''\n",
    "    # make everything lowercase\n",
    "    final_word = word.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    # Regex pattern that identifies all non letter or white space characters\n",
    "    punctuation_rule = r'[^\\w\\s]'\n",
    "    # removing punctuation\n",
    "    final_word = re.sub(punctuation_rule, '', final_word)\n",
    "    \n",
    "    # Remove stop words\n",
    "    final_word = strip_stopwords(final_word)\n",
    "\n",
    "    # Strip trailing white spaces\n",
    "    final_word = final_word.strip()\n",
    "    \n",
    "    # just need to lowercase everything, then done reformatting!\n",
    "    return final_word\n",
    "\n",
    "def spell_correct_country(phrase):\n",
    "      '''\n",
    "      Tries to spell correct country words using textblob.\n",
    "      Since textblob is not very good at spell checking phrases, we'll have\n",
    "      to split up country names into individual words if they're longer\n",
    "      than one word\n",
    "      \n",
    "      Will return a valid country word if it can, otherwise it will return NaN\n",
    "      '''\n",
    "      world_countries = list((pd.read_csv(\"World-Countries.csv\")).iloc[:, 0])\n",
    "      words = phrase.split()\n",
    "      if (len(words)) == 1:\n",
    "            # we only need to correct one word\n",
    "            \n",
    "            # here's some potential corrections\n",
    "            correction_options = [x[0] for x in Word(words[0]).spellcheck()]\n",
    "            # make sure the corrected words in the format we want\n",
    "            correction_options = [reformat_word(x) for x in correction_options]\n",
    "            \n",
    "            # see if any of the correction options are in our countries\n",
    "            for option in correction_options:\n",
    "                  if option in world_countries:\n",
    "                        # one of the correction options is a valid country\n",
    "                        # return this option\n",
    "                        return option\n",
    "                  else:\n",
    "                        # we couldn't find a valid correction :(\n",
    "                        return np.nan\n",
    "      else:\n",
    "            # we need to correct each word individually\n",
    "            corrected_words = []\n",
    "            for word in words:\n",
    "                  # find the best spelling correction of the word\n",
    "                  corrected_word = Word(word).correct()\n",
    "                  # reformat to lowercases, removed punctuation\n",
    "                  # and add it to the corrected words list\n",
    "                  corrected_words.append(reformat_word(corrected_word))\n",
    "            # now we recombine all the corrected words\n",
    "            final_correction = ' '.join(corrected_words)\n",
    "            # check if the final corrected word is a valid country and return\n",
    "            if final_correction in world_countries:\n",
    "                  # yes! the correction is a valid country\n",
    "                  return final_correction\n",
    "            else:\n",
    "                  # after everything, we still couldn't obtain a valid country\n",
    "                  return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function to clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_countries(user_data):\n",
    "      '''\n",
    "      cleans up the country feature in a user_data dataframe and returns the \n",
    "      cleaned dataframe\n",
    "      '''\n",
    "      # Make a dictionary of common country acronyms or alternative spellings after \n",
    "      # a quick observation of the data\n",
    "      with open(\"popular_alternative_country_names.json\", \"r\") as file:\n",
    "            alt_country_names = json.load(file)\n",
    "\n",
    "      # Add a cloumn to the dataframe verifying if each record's country entry is\n",
    "      # cleaned\n",
    "      user_data[\"Clean-Complete\"] = False\n",
    "\n",
    "      # here's how we'll call the records with country entries yet to clean \n",
    "      uncleaned_condition = user_data[\"Clean-Complete\"] == False\n",
    "\n",
    "      # MARK ANY RECORDS WE DON'T NEED TO FIX\n",
    "\n",
    "      # Any country entries that are NaN will be considered complete, we won't\n",
    "      # try impute the country here as it only adds confusion to our analysis\n",
    "      # and won't make the data easier to work with\n",
    "      user_data.loc[user_data[\"User-Country\"].isna(),\"Clean-Complete\"] = True\n",
    "      # reupdate the uncleaned_condition\n",
    "      uncleaned_condition = user_data[\"Clean-Complete\"] == False\n",
    "\n",
    "      # REFORMAT REMAINING RECORDS TO BE EASIER TO WORK WITH\n",
    "\n",
    "      # We'll now reformat the country entries to have all punctuation and \n",
    "      # trailing space stripped, turn ampersands into the word \"and\" and \n",
    "      # lowercase everything\n",
    "      user_data.loc[uncleaned_condition, \"User-Country\"] = user_data.loc[\n",
    "                  uncleaned_condition, \"User-Country\"].apply(str).apply(\n",
    "                                                            reformat_word)\n",
    "\n",
    "      # Check the country entries to see if they're a valid country. If yes, \n",
    "      # change \"clean-Complete\" tag to True\n",
    "      valid_country_condition = user_data[\"User-Country\"].isin(world_countries_s)\n",
    "      user_data.loc[uncleaned_condition & valid_country_condition, \n",
    "                    \"Clean-Complete\"] = True\n",
    "      # reupdate the uncleaned_condition\n",
    "      uncleaned_condition = user_data[\"Clean-Complete\"] == False\n",
    "\n",
    "      ## CONVERTING ACRONYMS TO FULL COUNTRY NAMES\n",
    "\n",
    "      # Find and convert as many acronym country entries into the formal country name\n",
    "      # Here's how we'll index for acronymed entries\n",
    "      acronym_detected_condition = user_data[\"User-Country\"].apply(lambda x: x in \n",
    "                                                                  alt_country_names)\n",
    "      # Now we convert the acronym\n",
    "      user_data.loc[acronym_detected_condition, \"User-Country\"] = user_data.loc[\n",
    "            acronym_detected_condition, \"User-Country\"].apply(\n",
    "                  lambda x: alt_country_names[x])\n",
    "      # Tag the record as \"clean-Complete\"=True\n",
    "      user_data.loc[acronym_detected_condition, \"Clean-Complete\"] = True\n",
    "      # reupdate the uncleaned_condition\n",
    "      uncleaned_condition = user_data[\"Clean-Complete\"] == False\n",
    "\n",
    "      ## SPELL CHECK TIME! (Last step!)\n",
    "      # try to spell check all the remaining entries\n",
    "      user_data.loc[uncleaned_condition, \"User-Country\"] = user_data.loc[\n",
    "            uncleaned_condition, \"User-Country\"].apply(spell_correct_country)\n",
    "\n",
    "      # Remove the \"Clean-Complete\" column, we don't need it anymore\n",
    "      user_data = user_data.drop(\"Clean-Complete\", axis=1)\n",
    "      \n",
    "      # Return the cleaned country data\n",
    "      return user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we'll actually clean the countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusers_df = clean_countries(nusers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Book Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity(mean_rating, total_ratings, mean_review_freq):\n",
    "    '''\n",
    "    returns the popularity metric of a book given the mean rating of that book, \n",
    "    the total number of ratings that book has, and the mean number of ratings\n",
    "    the books in the dataset has\n",
    "    '''\n",
    "    ## we chose 20 because we like its slow growth\n",
    "    return mean_rating * log(total_ratings+mean_review_freq)/log(20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out the mean_review_freq we'll use to calculate the popularities of each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1 unqiue frequency value for the number of ratings a certain book has\n",
      "Hence, every book has [3] ratings\n"
     ]
    }
   ],
   "source": [
    "# find the number of ratings per isbn\n",
    "num_ratings_per_isbn = nratings_df.groupby(\"ISBN\")[\"Book-Rating\"].size()\n",
    "\n",
    "# convert to a dataframe\n",
    "num_ratings_per_isbn_df = num_ratings_per_isbn.reset_index()\n",
    "num_ratings_per_isbn_df.columns = [\"ISBN\", \"Num-Ratings\"]\n",
    "\n",
    "# the unique \"Num-Ratings\" entries are:\n",
    "unqiue_numratings_entries = num_ratings_per_isbn_df['Num-Ratings'].unique()\n",
    "\n",
    "print(f\"There is {len(unqiue_numratings_entries)} unqiue frequency value for \" \n",
    "      \"the number of ratings a certain book has\")\n",
    "print(f\"Hence, every book has {unqiue_numratings_entries} ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each book has the same number of ratings, it hints that the dataset may not be an accurate representation of how many people actually read and review each book, but is rather strategically accumilated such that there are 3 ratings per book.\n",
    "\n",
    "Because of this, the popularity score of each book may not be an accurate representation of how popular the book is. We will thus not use the new_data as a testing set and will stop cleaning it here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
